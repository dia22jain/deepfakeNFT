{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlfDXY-tw7ec"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')              #here we mount the files present in our google drive to the google colab Files folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mkDpUoFx74t"
      },
      "outputs": [],
      "source": [
        "import gdown                     #used for downloading files from google drive\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Google Drive file ID\n",
        "file_id = '1OOtsqCY4zyNLF4fdNBJWM2948n6w_elN'\n",
        "# https://drive.google.com/file/d/1OOtsqCY4zyNLF4fdNBJWM2948n6w_elN/view?usp=drive_link\n",
        "\n",
        "# Construct the download URL\n",
        "url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "# Define the output file name (folder name)\n",
        "output = 'DataSet.zip'\n",
        "\n",
        "# Download the file with gdown\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Define the directory where you want to unzip the files\n",
        "unzip_dir = '/content/unzipped_files'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(unzip_dir, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall(unzip_dir)\n",
        "\n",
        "print(f'Unzipped files are stored in: {unzip_dir}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK3PwnAOxeRN"
      },
      "outputs": [],
      "source": [
        "import glob                               # Provides functions to find all pathnames matching a specified pattern.\n",
        "import numpy as np                            # A library for numerical operations, particularly useful for statistical calculations.\n",
        "import cv2                                    #open CV library used for video processing\n",
        "import os                                 # Provides a way to interact with the operating system, here used for path manipulations\n",
        "\n",
        "# Define the base directory where the video files are located\n",
        "base_dir = '/content/unzipped_files'\n",
        "\n",
        "# Gather video files from multiple directories('**' used for recursive searching, *.mp4- looks for mp4 files)\n",
        "video_files = glob.glob(os.path.join(base_dir, '**', '*.mp4'), recursive=True)\n",
        "\n",
        "# Initialize an empty list for frame counts\n",
        "frame_count = []\n",
        "\n",
        "# Process each video file\n",
        "for video_file in video_files:\n",
        "    cap = cv2.VideoCapture(video_file)              #a VideoCapture object to read the video file\n",
        "\n",
        "    # Check if video file was opened successfully\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error opening video file {video_file}\")\n",
        "        continue\n",
        "\n",
        "    # Get the number of frames in the video\n",
        "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Only consider videos with at least 150 frames(the limit 150 helps in serving as a filter to exclude videos that might be too short to provide meaningful analysis\n",
        "    if num_frames >= 150:\n",
        "        frame_count.append(num_frames)\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "# Check if there are valid frame counts\n",
        "if frame_count:\n",
        "    print(\"Frames:\", frame_count)\n",
        "    print(\"Total number of videos:\", len(frame_count))\n",
        "    print('Average frames per video:', np.mean(frame_count))\n",
        "else:\n",
        "    print(\"No valid videos found with at least 150 frames.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w13wWbpt1Q7O"
      },
      "outputs": [],
      "source": [
        "!pip install face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fczoaMeuXPiT"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "import face_recognition                            # Library for face detection and recognition tasks\n",
        "from tqdm.autonotebook import tqdm                # A library for creating progress bars, useful for visualizing the progress of long-running tasks\n",
        "\n",
        "# Function to extract frames from a video\n",
        "def frame_extract(path):\n",
        "    vidObj = cv2.VideoCapture(path)          # Create a VideoCapture object to read the video file\n",
        "    success = True\n",
        "    while success:                                 # Looping until all frames have been read\n",
        "        success, image = vidObj.read()          #reading frames\n",
        "        if success:\n",
        "            yield image\n",
        "\n",
        "# Create output directories for face-only data (real and fake)\n",
        "real_output_dir = '/content/drive/My Drive/FF_REAL_FACE_only'\n",
        "fake_output_dir = '/content/drive/My Drive/FF_FAKE_FACE_only'\n",
        "\n",
        "if not os.path.exists(real_output_dir):\n",
        "    os.makedirs(real_output_dir)\n",
        "\n",
        "if not os.path.exists(fake_output_dir):\n",
        "    os.makedirs(fake_output_dir)\n",
        "\n",
        "# Process the frames and extract faces\n",
        "def create_face_videos(path_list, real_dir, fake_dir):\n",
        "    real_videos_count = glob.glob(real_dir + '/*.mp4')\n",
        "    fake_videos_count = glob.glob(fake_dir + '/*.mp4')\n",
        "\n",
        "    print(\"No. of real videos already present:\", len(real_videos_count))\n",
        "    print(\"No. of fake videos already present:\", len(fake_videos_count))\n",
        "\n",
        "    # Iterate through each video in the path list\n",
        "    for path in tqdm(path_list):\n",
        "        # Determine if the video is real or fake based on the folder name\n",
        "        if 'celeb-real' in path.lower():\n",
        "            out_dir = real_dir\n",
        "        elif 'celeb-synthesis' in path.lower():\n",
        "            out_dir = fake_dir\n",
        "        else:\n",
        "            print(f\"Skipping unknown video category for {path}\")\n",
        "            continue\n",
        "\n",
        "        out_path = os.path.join(out_dir, os.path.basename(path))       # Defining the output path where the processed video will be saved\n",
        "        file_exists = glob.glob(out_path)\n",
        "\n",
        "        # Skip if the file already exists\n",
        "        if len(file_exists) != 0:\n",
        "            print(\"File already exists:\", out_path)\n",
        "            continue\n",
        "\n",
        "        # Open the video file\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Error opening video file {path}\")\n",
        "            continue\n",
        "\n",
        "        frames = []            #storing frames for processing\n",
        "        out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), 30, (112, 112))       # Prepare the output video writer with codec (MJPG), frame rate (30), and frame size (112x112)\n",
        "\n",
        "        for idx, frame in enumerate(frame_extract(path)):    #extract frames from video\n",
        "            if idx <= 150:  # Only process the first 150 frames\n",
        "                frames.append(frame)\n",
        "\n",
        "                if len(frames) == 4:  # Process every 4 frames for face recognition(to improve efficiency and optimize memory usage)\n",
        "                    faces = face_recognition.batch_face_locations(frames)\n",
        "                    for i, face in enumerate(faces):     #iterating through detected faces & crop them\n",
        "                        if len(face) != 0:\n",
        "                            top, right, bottom, left = face[0]      #coordinates of face\n",
        "                            try:\n",
        "                                # Write the detected face to the video by resizing it to 112*112 pixels\n",
        "                                face_frame = cv2.resize(frames[i][top:bottom, left:right], (112, 112))\n",
        "                                out.write(face_frame)\n",
        "                            except Exception as e:\n",
        "                                print(f\"Error processing frame {idx}:\", e)\n",
        "                    frames = []\n",
        "\n",
        "        out.release()\n",
        "        cap.release()\n",
        "\n",
        "# Define the base directory and gather all .mp4 video files from all folders\n",
        "base_dir = '/content/unzipped_files'\n",
        "# Modified glob to find all .mp4 files recursively\n",
        "video_files = glob.glob(os.path.join(base_dir, '**/*.mp4'), recursive=True)\n",
        "\n",
        "# Check if videos were found\n",
        "if not video_files:\n",
        "    print(\"No videos found!\")\n",
        "\n",
        "# Process the video files and save output in the respective directories\n",
        "create_face_videos(video_files, real_output_dir, fake_output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzYtWnzJNY8S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AHPYQ3B8oor"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import torch              # PyTorch for deep learning tasks\n",
        "import torchvision             # PyTorch's vision library for working with image datasets and models\n",
        "from torchvision import transforms          # For applying transformations to images\n",
        "from torch.utils.data import DataLoader           # DataLoader and Dataset classes for handling data\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import face_recognition\n",
        "\n",
        "# Check if the video is corrupted or not\n",
        "def validate_video(vid_path, train_transforms):\n",
        "\n",
        "    \"\"\"\n",
        "    Takes the video path and transformation function as input,\n",
        "    extracts frames, applies transformations, and checks the integrity of the video.\n",
        "    \"\"\"\n",
        "\n",
        "    transform = train_transforms          #setting transformations to validate video\n",
        "    count = 20              #no of frames to be extracted from video\n",
        "    frames = []                   # List to store the frames after transformation\n",
        "    a = int(100 / count)                # Step size for extracting frames\n",
        "    first_frame = np.random.randint(0, a)               #Randomly selecting a starting frame\n",
        "    temp_video = vid_path.split('/')[-1]\n",
        "\n",
        "    for i, frame in enumerate(frame_extract(vid_path)):             #looping through each extracted frame\n",
        "        frames.append(transform(frame))                       #applying transformation\n",
        "        if len(frames) == count:\n",
        "            break\n",
        "\n",
        "    # Stack frames to check the validity\n",
        "    frames = torch.stack(frames)\n",
        "    frames = frames[:count]\n",
        "    return frames\n",
        "\n",
        "# Extract a frame from the video\n",
        "def frame_extract(path):\n",
        "    vidObj = cv2.VideoCapture(path)             #opening video file\n",
        "    success = True\n",
        "    while success:\n",
        "        success, image = vidObj.read()\n",
        "        if success:\n",
        "            yield image\n",
        "\n",
        "# Image processing and normalization\n",
        "#mean and standard deviation are used here to normalize the images (frames) as part of the preprocessing step\n",
        "im_size = 112\n",
        "mean = [0.485, 0.456, 0.406]              # Mean for normalization (standard ImageNet mean)\n",
        "std = [0.229, 0.224, 0.225]                  # Standard deviation for normalization (standard ImageNet std)\n",
        "\n",
        "#defining the transformations to be applied to the video\n",
        "#tensors allow for fast and efficient matrix operations, which are essential for the computations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),                    # Converting NumPy array (from OpenCV) to PIL image(PIL is a lib for image processing)\n",
        "    transforms.Resize((im_size, im_size)),                 # Resizing the image to (112x112)\n",
        "    transforms.ToTensor(),                       # Converting PIL image to PyTorch tensor(multi-dimensional arrays)\n",
        "    transforms.Normalize(mean, std)                      # Normalizing the image with the given mean and std\n",
        "])\n",
        "\n",
        "# List of video directories to check for corrupted files\n",
        "video_dirs = [\n",
        "\n",
        "    '/content/drive/My Drive/FF_FAKE_FACE_only/*.mp4',\n",
        "    '/content/drive/My Drive/FF_REAL_FACE_only/*.mp4'\n",
        "]\n",
        "\n",
        "# Gather all .mp4 video files from the directories\n",
        "video_files = []\n",
        "for video_dir in video_dirs:\n",
        "    video_files += glob.glob(video_dir)\n",
        "\n",
        "print(\"Total number of videos:\", len(video_files))\n",
        "print(video_files)\n",
        "\n",
        "# Loop through each video and validate\n",
        "count = 0              #counter to track each the number of processed videos\n",
        "for video_path in video_files:\n",
        "    try:\n",
        "        count += 1\n",
        "        validate_video(video_path, train_transforms)\n",
        "    except Exception as e:\n",
        "        print(\"Number of videos processed:\", count, \"Remaining:\", (len(video_files) - count))\n",
        "        print(\"Corrupted video is:\", video_path)\n",
        "        print(\"Error details:\", str(e))\n",
        "        # Remove corrupted video\n",
        "        os.remove(video_path)\n",
        "        print(f\"Deleted corrupted video: {video_path}\")\n",
        "        continue\n",
        "\n",
        "print(\"Number of corrupted videos deleted:\", len(video_files) - count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFU70gsnNcrd"
      },
      "outputs": [],
      "source": [
        "import json             # For handling JSON data (not used here)\n",
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import copy                  # For creating deep copies of objects(not used here)\n",
        "import random\n",
        "video_files =  glob.glob('/content/drive/My Drive/FF_FAKE_FACE_only/*.mp4')\n",
        "video_files += glob.glob('/content/drive/My Drive/FF_REAL_FACE_only/*.mp4')\n",
        "\n",
        "random.shuffle(video_files)\n",
        "random.shuffle(video_files)           #two shuffles to ensure randomness\n",
        "\n",
        "frame_count = []                #list for storing frame counts in each video\n",
        "for video_file in video_files:\n",
        "  cap = cv2.VideoCapture(video_file)               #opening video file\n",
        "  if(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))<100):            #if video has less than 100 frames, we remove it\n",
        "    video_files.remove(video_file)\n",
        "    continue\n",
        "  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
        "print(\"frames are \" , frame_count)\n",
        "print(\"Total no of video: \" , len(frame_count))\n",
        "print('Average frame per video:',np.mean(frame_count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zfNCpPAeXxg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import csv\n",
        "\n",
        "# Define the base directory where the video files are located\n",
        "base_dir = '/content/unzipped_files'\n",
        "\n",
        "# Define paths for real and fake video folders\n",
        "real_videos_path = os.path.join(base_dir, 'Celeb-real')\n",
        "fake_videos_path = os.path.join(base_dir, 'Celeb-synthesis')\n",
        "\n",
        "# Find all .mp4 files in real and fake folders\n",
        "real_videos = glob.glob(os.path.join(real_videos_path, '**/*.mp4'), recursive=True)\n",
        "fake_videos = glob.glob(os.path.join(fake_videos_path, '**/*.mp4'), recursive=True)\n",
        "\n",
        "# Define the output CSV file path\n",
        "output_csv_path = '/content/video_labels.csv'\n",
        "\n",
        "# Create and write to the CSV file\n",
        "with open(output_csv_path, mode='w', newline='') as file:      #in write mode with no new newline\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow(['Video Name', 'Label'])\n",
        "\n",
        "    # Write real video entries\n",
        "    for video in real_videos:\n",
        "        video_name = os.path.basename(video)\n",
        "        writer.writerow([video_name, 'REAL'])\n",
        "\n",
        "    # Write fake video entries\n",
        "    for video in fake_videos:\n",
        "        video_name = os.path.basename(video)\n",
        "        writer.writerow([video_name, 'FAKE'])\n",
        "\n",
        "print(f\"CSV file created at: {output_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sdf2i1n5VQXo"
      },
      "outputs": [],
      "source": [
        "import torch                    # PyTorch for deep learning tasks\n",
        "import torchvision\n",
        "from torchvision import transforms                 # Image transformation utilities\n",
        "from torch.utils.data import DataLoader           # For creating batches of data\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd  # Import pandas to handle CSV files\n",
        "import face_recognition  # Optional import, though it's not used in the current code\n",
        "\n",
        "# Define the video dataset class\n",
        "class video_dataset(Dataset):\n",
        "    def __init__(self, video_names, labels, sequence_length=60, transform=None):\n",
        "        self.video_names = video_names  # List of video file paths\n",
        "        self.labels = labels  # DataFrame containing the video names and labels\n",
        "        self.transform = transform  # Transform to apply to frames (e.g., resizing, normalization)\n",
        "        self.count = sequence_length  # Number of frames to extract per video\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_names)  # Return the number of videos\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_names[idx]  # Get the video file path\n",
        "        frames = []  # List to store the extracted frames\n",
        "        a = int(100 / self.count)  # To control frame skipping\n",
        "        first_frame = np.random.randint(0, a)\n",
        "        temp_video = video_path.split('/')[-1]  # Get the video file name without the full path\n",
        "\n",
        "        # Get the label for the current video based on the CSV data\n",
        "        label = self.labels[self.labels[\"Video Name\"] == temp_video][\"Label\"].values[0]\n",
        "        label = 0 if label == 'FAKE' else 1  # Convert 'FAKE' to 0 and 'REAL' to 1\n",
        "\n",
        "        # Extract and transform frames\n",
        "        for i, frame in enumerate(self.frame_extract(video_path)):\n",
        "            frames.append(self.transform(frame))  # Apply transformations (resize, tensor, etc.)\n",
        "            if len(frames) == self.count:\n",
        "                break\n",
        "\n",
        "        frames = torch.stack(frames)  # Convert the list of frames to a tensor\n",
        "        frames = frames[:self.count]  # Ensure we only take the desired number of frames\n",
        "        return frames, label  # Return the frames and the corresponding label\n",
        "\n",
        "    def frame_extract(self, path):                       #function to extract frames from the video\n",
        "        vidObj = cv2.VideoCapture(path)  # Open the video file\n",
        "        success = True\n",
        "        while success:\n",
        "            success, image = vidObj.read()  # Read a frame\n",
        "            if success:\n",
        "                yield image  # Yield the frame\n",
        "\n",
        "# Function to plot a tensor image\n",
        "def im_plot(tensor):\n",
        "    image = tensor.cpu().numpy().transpose(1, 2, 0)  # Convert tensor to numpy array and rearrange the dimensions for plotting\n",
        "    b, g, r = cv2.split(image)  # Split channels (OpenCV uses BGR format)\n",
        "    image = cv2.merge((r, g, b))  # Convert BGR to RGB\n",
        "    # Undo normalization (assuming specific mean and std values)\n",
        "    image = image * [0.22803, 0.22145, 0.216989] + [0.43216, 0.394666, 0.37645]\n",
        "    image = image * 255.0  # Scale back to the range of pixel values\n",
        "    plt.imshow(image.astype(int))  # Plot the image\n",
        "    plt.show()\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file_path = '/content/video_labels.csv'\n",
        "labels = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Get the list of video file paths from two folders\n",
        "video_folder_1 = '/content/drive/MyDrive/FF_REAL_FACE_only'  # First folder containing videos\n",
        "video_folder_2 = '/content/drive/MyDrive/FF_FAKE_FACE_only'  # Second folder containing videos\n",
        "\n",
        "# Read videos from both folders and combine them into one list\n",
        "video_names_1 = [os.path.join(video_folder_1, f) for f in os.listdir(video_folder_1) if f.endswith('.mp4')]  # Adjust for your video format\n",
        "video_names_2 = [os.path.join(video_folder_2, f) for f in os.listdir(video_folder_2) if f.endswith('.mp4')]\n",
        "\n",
        "# Merge the lists from both folders\n",
        "video_names = video_names_1 + video_names_2\n",
        "\n",
        "# Define transformation pipeline for frames\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),          # Convert frame to PIL image\n",
        "    transforms.Resize((224, 224)),    # Resize the frame\n",
        "    transforms.ToTensor()             # Convert image to tensor\n",
        "])\n",
        "\n",
        "# Initialize the dataset\n",
        "sequence_length = 60  # Set the number of frames to extract from each video\n",
        "dataset = video_dataset(video_names=video_names, labels=labels, sequence_length=sequence_length, transform=transform)\n",
        "\n",
        "# Example usage:\n",
        "# Get a sample from the dataset (for instance, the first video)\n",
        "frames, label = dataset[82]  # Fetch the first video frames and label\n",
        "print(\"Frames shape:\", frames.shape)  # Should print something like (60, 3, 224, 224) if 60 frames are extracted\n",
        "print(\"Label:\", label)  # Will print 0 (FAKE) or 1 (REAL)\n",
        "\n",
        "# Plot a single frame from the sequence\n",
        "im_plot(frames[0])  # Plot the first frame from the extracted frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_-do-6CZN0N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def number_of_real_and_fake_videos(data_list):\n",
        "    # Load the CSV with the correct header\n",
        "    header_list = [\"Video Name\", \"Label\"]\n",
        "    lab = pd.read_csv('/content/video_labels.csv', names=header_list)\n",
        "\n",
        "    fake = 0\n",
        "    real = 0\n",
        "\n",
        "    # Loop through the video list\n",
        "    for i in data_list:\n",
        "        temp_video = i.split('/')[-1]  # Get the video filename\n",
        "        # Find the label for the current video based on the Video Name\n",
        "        label = lab.iloc[(lab.loc[lab[\"Video Name\"] == temp_video].index.values[0]), 1]  # Get the label\n",
        "\n",
        "        # Count based on the label\n",
        "        if label == 'FAKE':\n",
        "            fake += 1\n",
        "        if label == 'REAL':\n",
        "            real += 1\n",
        "\n",
        "    # Print after counting all videos\n",
        "    print(\"Number of fake videos: \", fake)\n",
        "    print(\"Number of real videos: \", real)\n",
        "\n",
        "    return real, fake\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd0IFSj6tI9U"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split              # Import train_test_split for data splitting\n",
        "\n",
        "# Load the CSV with the correct column names\n",
        "labels = pd.read_csv('/content/video_labels.csv')  # CSV already has the headers\n",
        "\n",
        "# Split video files into training and validation sets (80% for training, 20% for validation)\n",
        "train_videos = video_files[:int(0.7 * len(video_files))]\n",
        "valid_videos = video_files[int(0.7 * len(video_files)):]\n",
        "\n",
        "print(\"train : \", len(train_videos))\n",
        "print(\"test : \", len(valid_videos))\n",
        "\n",
        "# Print the number of real and fake videos in the training and validation sets\n",
        "print(\"TRAIN: \", \"Real:\", number_of_real_and_fake_videos(train_videos)[0], \" Fake:\", number_of_real_and_fake_videos(train_videos)[1])\n",
        "print(\"TEST: \", \"Real:\", number_of_real_and_fake_videos(valid_videos)[0], \" Fake:\", number_of_real_and_fake_videos(valid_videos)[1])\n",
        "\n",
        "# Define image size and normalization parameters\n",
        "im_size = 112\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Define transformations for training and validation data\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((im_size, im_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((im_size, im_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Update video_dataset to work with the correct labels\n",
        "train_data = video_dataset(train_videos, labels, sequence_length=10, transform=train_transforms)\n",
        "val_data = video_dataset(valid_videos, labels, sequence_length=10, transform=test_transforms)\n",
        "\n",
        "# Create DataLoader for training and validation\n",
        "train_loader = DataLoader(train_data, batch_size=4, shuffle=True, num_workers=4)\n",
        "valid_loader = DataLoader(val_data, batch_size=4, shuffle=True, num_workers=4)\n",
        "\n",
        "# Example: Access an image and label from the dataset\n",
        "image, label = train_data[0]\n",
        "im_plot(image[0, :, :, :])  # Plot the first frame of the sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8ysbKR4Bmgd"
      },
      "outputs": [],
      "source": [
        " # Instantiate the model with 2 output classes (for binary classification) and moved the model to the GPU for faster processing using CUDA.\n",
        "model = Model(2).cuda()\n",
        "\n",
        "# Create an uninitialized tensor (1 video, 20 frames, 3 channels, 112x112),convert to CUDA FloatTensor, and pass to the model, which returns two outputs.\n",
        "a,b = model(torch.from_numpy(np.empty((1,20,3,112,112))).type(torch.cuda.FloatTensor))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iev_mH--k5d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch                             #PyTorch library for tensor operations and deep learning.\n",
        "import torch.nn as nn                    #Imports the neural network module from PyTorch, providing essential layers and loss functions.\n",
        "from torchvision import models\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import seaborn as sn                    #for advanced data visualization and plotting.\n",
        "import matplotlib.pyplot as plt         #for creating static, animated, and interactive visualizations.\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Model Definition\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=True):     #latent_dim= dimension of input feature to LSTM\n",
        "        super(Model, self).__init__()                                                                         #Calls the constructor of the parent class (nn.Module) to ensure proper initialization of the neural network.\n",
        "        model = models.resnext50_32x4d(pretrained=True)                                                       #Loads a pre-trained ResNeXt-50 model\n",
        "        self.model = nn.Sequential(*list(model.children())[:-2])                                              #Extracts all layers except the last two (typically the classification head) to use as a feature extractor.\n",
        "        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)\n",
        "        self.relu = nn.LeakyReLU()                                                                            #Initializes a Leaky ReLU activation function\n",
        "        self.dp = nn.Dropout(0.4)                                                                             #Adds a dropout layer with a rate of 0.4 to help prevent overfitting by randomly zeroing out 40% of the neurons during training.\n",
        "        self.linear1 = nn.Linear(2048, num_classes)                                                           #Defines a fully connected layer that maps the LSTM output to the number of classes (2 classes: fake and real).\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)                                                                #Initializes an adaptive average pooling layer that reduces the spatial dimensions of the feature maps to 1x1, retaining global information.\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, c, h, w = x.shape                                                             #c=no. of channels (3-RGB) (h*w=112*112)\n",
        "        x = x.view(batch_size * seq_length, c, h, w)                                                          #Reshapes the input tensor x to (batch_size * seq_length, channels, height, width), allowing each frame to be processed individually by the CNN.\n",
        "        fmap = self.model(x)                                                                                  #Passes the reshaped input through the ResNeXt model to extract feature maps.\n",
        "        x = self.avgpool(fmap)                                                                                #Applies adaptive average pooling to reduce spatial dimensions to 1x1.\n",
        "        x = x.view(batch_size, seq_length, 2048)                                                              #Reshapes the pooled output to (batch_size, seq_length, 2048), making it suitable for input to the LSTM.\n",
        "        x_lstm, _ = self.lstm(x, None)                                                                        #Passes the input tensor through the LSTM layer, initializing the hidden state to zeros; 'x_lstm' contains the LSTM output for each time step and output of hidden state is ignored(_).\n",
        "        return fmap, self.dp(self.linear1(torch.mean(x_lstm, dim=1)))                                         #outputs:1) Feature maps from the CNN. 2) The final classification scores after dropout of linear layer applied after averaging LSTM outputs.\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train_epoch(epoch, num_epochs, data_loader, model, criterion, optimizer):\n",
        "    print('Training')\n",
        "    model.train()                                                                                            #Enables training-specific behaviors like dropout and batch normalization.\n",
        "\n",
        "    #avg loss and accuracy\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(data_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            labels = labels.type(torch.cuda.LongTensor).cuda()\n",
        "            inputs = inputs.cuda()\n",
        "\n",
        "        _, outputs = model(inputs)                                                                          #Passes inputs through the model to get outputs.\n",
        "        loss = criterion(outputs, labels)                                                                   #loss between the model outputs and true labels using the specified loss criterion.\n",
        "        acc = calculate_accuracy(outputs, labels)                                                           #accuracy for the batch.\n",
        "\n",
        "        #Updates the losses and accuracies meters with the current batch's loss and accuracy.\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "        accuracies.update(acc, inputs.size(0))\n",
        "\n",
        "        # Zero the optimizer's gradients, compute gradients through backpropagation using the loss, and update model parameters based on the computed gradients.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        sys.stdout.write(\n",
        "            \"\\r[Epoch %d/%d] [Batch %d / %d] [Loss: %f, Acc: %.2f%%]\"\n",
        "            % (epoch, num_epochs, i, len(data_loader), losses.avg, accuracies.avg))\n",
        "\n",
        "    #Saves the model's state after the epoch.\n",
        "    torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "\n",
        "    return losses.avg, accuracies.avg                                                                       #Returns the average loss and accuracy for the epoch.\n",
        "\n",
        "\n",
        "def test(epoch, model, data_loader, criterion):\n",
        "    print('Testing')\n",
        "\n",
        "    #Put the model in evaluation mode, disabling dropout and batch normalization.\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    #avg loss and accuracy\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "\n",
        "    #empty lists to store predictions (pred) and true labels (true).\n",
        "    pred = []\n",
        "    true = []\n",
        "\n",
        "    with torch.no_grad():                                                                                   #Disables gradient computation for efficiency.\n",
        "        for i, (inputs, labels) in enumerate(data_loader):\n",
        "            if torch.cuda.is_available():\n",
        "                labels = labels.cuda().type(torch.cuda.LongTensor)\n",
        "                inputs = inputs.cuda()\n",
        "\n",
        "            #Passes the inputs through the model to get predictions.\n",
        "            _, outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            acc = calculate_accuracy(outputs, labels)\n",
        "\n",
        "            #Gets the predicted class labels by finding the index of the maximum score in the outputs.\n",
        "            _, p = torch.max(outputs, 1)\n",
        "\n",
        "            #Appends the true and predicted labels to their respective lists, converting them to NumPy arrays first.\n",
        "            true += labels.cpu().numpy().reshape(len(labels)).tolist()\n",
        "            pred += p.cpu().numpy().reshape(len(p)).tolist()\n",
        "\n",
        "            #Updates the loss and accuracy meters with the current batch's loss and accuracy.\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "            accuracies.update(acc, inputs.size(0))\n",
        "\n",
        "            sys.stdout.write(\n",
        "                \"\\r[Batch %d / %d]  [Loss: %f, Acc: %.2f%%]\"\n",
        "                % (i+1, len(data_loader), losses.avg, accuracies.avg)\n",
        "            )\n",
        "\n",
        "        #Print the average accuracy for the test set after all batches have been processed.\n",
        "        print('\\nAccuracy {}'.format(accuracies.avg))\n",
        "\n",
        "    return true, pred, losses.avg, accuracies.avg                                                        #Returns the true labels, predicted labels, average loss, and average accuracy.\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def calculate_accuracy(outputs, labels):\n",
        "    batch_size = labels.size(0)                         #Number of samples in the batch.\n",
        "    _, pred = outputs.topk(1, 1, True)                  #Retrieves the indices of the top-1 predictions for each sample.\n",
        "    pred = pred.t()                                     #Transposes the prediction tensor for comparison.\n",
        "    correct = pred.eq(labels.view(1, -1))               #Checks if predictions match true labels.\n",
        "    n_correct_elems = correct.float().sum().item()      #Counts the number of correct predictions.\n",
        "    return 100 * n_correct_elems / batch_size\n",
        "\n",
        "# Plotting Functions\n",
        "def print_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    df_cm = pd.DataFrame(cm, range(2), range(2))\n",
        "    sn.set(font_scale=1.4)\n",
        "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16})\n",
        "    plt.ylabel('Actual label', size=20)\n",
        "    plt.xlabel('Predicted label', size=20)\n",
        "    plt.xticks(np.arange(2), ['Fake', 'Real'], size=16)\n",
        "    plt.yticks(np.arange(2), ['Fake', 'Real'], size=16)\n",
        "    plt.ylim([2, 0])\n",
        "    plt.show()\n",
        "    calculated_acc = (cm[0][0] + cm[1][1]) / (cm[0][0] + cm[0][1] + cm[1][0] + cm[1][1])\n",
        "    print(\"Calculated Accuracy\", calculated_acc * 100)\n",
        "\n",
        "def plot_loss(train_loss_avg, test_loss_avg, num_epochs):\n",
        "    epochs = range(1, num_epochs + 1)\n",
        "    plt.plot(epochs, train_loss_avg, 'g', label='Training loss')\n",
        "    plt.plot(epochs, test_loss_avg, 'b', label='Validation loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_accuracy(train_accuracy, test_accuracy, num_epochs):\n",
        "    epochs = range(1, num_epochs + 1)\n",
        "    plt.plot(epochs, train_accuracy, 'g', label='Training accuracy')\n",
        "    plt.plot(epochs, test_accuracy, 'b', label='Validation accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Hyperparameters and Initialization\n",
        "lr = 1e-5  # Learning rate\n",
        "num_epochs = 15  # Number of epochs\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5) #Initializes the Adam optimizer with the model parameters, specified learning rate, and weight decay for regularization.\n",
        "\n",
        "# If class imbalance exists\n",
        "class_weights = torch.tensor([1.0, 5.0]).cuda()  # Defines class weights to handle class imbalance\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights).cuda()  # Uses cross-entropy loss with class weights\n",
        "\n",
        "# Lists to store average loss and accuracy\n",
        "train_loss_avg = []\n",
        "train_accuracy = []\n",
        "test_loss_avg = []\n",
        "test_accuracy = []\n",
        "\n",
        "# Training and Validation Loop\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    l, acc = train_epoch(epoch, num_epochs, train_loader, model, criterion, optimizer)\n",
        "    train_loss_avg.append(l)\n",
        "    train_accuracy.append(acc)\n",
        "\n",
        "    true, pred, tl, t_acc = test(epoch, model, valid_loader, criterion)\n",
        "    test_loss_avg.append(tl)\n",
        "    test_accuracy.append(t_acc)\n",
        "\n",
        "# Plotting Loss and Accuracy\n",
        "plot_loss(train_loss_avg, test_loss_avg, len(train_loss_avg))\n",
        "plot_accuracy(train_accuracy, test_accuracy, len(train_accuracy))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(confusion_matrix(true, pred))\n",
        "print_confusion_matrix(true, pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9dOJKc4KskO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# Define the model (make sure the model class definition is included above)\n",
        "model = Model(2).cuda()\n",
        "\n",
        "# Path to the saved model\n",
        "path_to_model = '/content/checkpoint.pt'\n",
        "\n",
        "# Check if the model file exists\n",
        "if os.path.exists(path_to_model):\n",
        "    model.load_state_dict(torch.load(path_to_model))\n",
        "    model.eval()\n",
        "    print(\"Model loaded successfully from:\", path_to_model)\n",
        "else:\n",
        "    print(f\"Model file not found at {path_to_model}. Please check the path and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZUlJ6S2LilU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "\n",
        "# Normalization parameters\n",
        "im_size = 112\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "sm = nn.Softmax(dim=1)\n",
        "\n",
        "# Inverse normalization for image display\n",
        "inv_normalize = transforms.Normalize(mean=[-m/s for m, s in zip(mean, std)],\n",
        "                                      std=[1/s for s in std])\n",
        "\n",
        "def im_convert(tensor):\n",
        "    \"\"\"Display a tensor as an image.\"\"\"\n",
        "    image = tensor.to(\"cpu\").clone().detach()\n",
        "    image = image.squeeze()\n",
        "    image = inv_normalize(image)\n",
        "    image = image.numpy()\n",
        "    image = image.transpose(1, 2, 0)\n",
        "    image = image.clip(0, 1)\n",
        "    return (image * 255).astype(np.uint8)\n",
        "\n",
        "def predict(model, img, path='./'):\n",
        "    \"\"\"Make predictions and visualize the results.\"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    fmap, logits = model(img.to('cuda'))  # Get feature maps and logits\n",
        "    weight_softmax = model.linear1.weight.detach().cpu().numpy()  # Get weights for the softmax\n",
        "    logits = sm(logits)  # Apply softmax to logits\n",
        "    _, prediction = torch.max(logits, 1)  # Get the predicted class\n",
        "    confidence = logits[:, int(prediction.item())].item() * 100  # Confidence percentage\n",
        "\n",
        "    print('Confidence of prediction:', confidence)\n",
        "\n",
        "    idx = np.argmax(logits.detach().cpu().numpy())  # Get index of the class with the highest probability\n",
        "    bz, nc, h, w = fmap.shape  # Get dimensions of the feature map\n",
        "\n",
        "    # Compute the weighted sum of the feature map\n",
        "    out = np.dot(fmap[-1].detach().cpu().numpy().reshape((nc, h * w)).T, weight_softmax[idx, :].T)\n",
        "    predict_map = out.reshape(h, w)  # Reshape to original dimensions\n",
        "    predict_map = predict_map - np.min(predict_map)  # Normalize to [0, 1]\n",
        "    predict_img = predict_map / np.max(predict_map)\n",
        "    predict_img = np.uint8(255 * predict_img)  # Scale to [0, 255]\n",
        "    out_resized = cv2.resize(predict_img, (im_size, im_size))  # Resize heatmap\n",
        "\n",
        "    # Generate heatmap\n",
        "    heatmap = cv2.applyColorMap(out_resized, cv2.COLORMAP_JET)\n",
        "\n",
        "    # Convert the input image tensor to a visual format\n",
        "    img_display = im_convert(img[:, -1, :, :, :])  # Use the last frame for visualization\n",
        "\n",
        "    # Combine heatmap and original image\n",
        "    result = heatmap * 0.5 + img_display * 0.8\n",
        "    result_final = cv2.merge(cv2.split(result))  # Ensure the color channels are merged\n",
        "\n",
        "    # Save the resulting image\n",
        "    cv2.imwrite(path + 'result.png', result_final)\n",
        "    plt.imshow(result_final.astype(np.uint8))\n",
        "    plt.axis('off')  # Hide axes\n",
        "    plt.show()  # Display the result\n",
        "\n",
        "    return [int(prediction.item()), confidence]  # Return prediction and confidence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G_iJPCpL-uM"
      },
      "outputs": [],
      "source": [
        "#!pip3 install face_recognition\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import face_recognition\n",
        "class validation_dataset(Dataset):\n",
        "    def __init__(self,video_names,sequence_length = 60,transform = None):\n",
        "        self.video_names = video_names\n",
        "        self.transform = transform\n",
        "        self.count = sequence_length\n",
        "    def __len__(self):\n",
        "        return len(self.video_names)\n",
        "    def __getitem__(self,idx):\n",
        "        video_path = self.video_names[idx]\n",
        "        frames = []\n",
        "        a = int(100/self.count)\n",
        "        first_frame = np.random.randint(0,a)\n",
        "        for i,frame in enumerate(self.frame_extract(video_path)):\n",
        "            #if(i % a == first_frame):\n",
        "            faces = face_recognition.face_locations(frame)\n",
        "            try:\n",
        "              top,right,bottom,left = faces[0]\n",
        "              frame = frame[top:bottom,left:right,:]\n",
        "            except:\n",
        "              pass\n",
        "            frames.append(self.transform(frame))\n",
        "            if(len(frames) == self.count):\n",
        "              break\n",
        "        #print(\"no of frames\",len(frames))\n",
        "        frames = torch.stack(frames)\n",
        "        frames = frames[:self.count]\n",
        "        return frames.unsqueeze(0)\n",
        "    def frame_extract(self,path):\n",
        "      vidObj = cv2.VideoCapture(path)\n",
        "      success = 1\n",
        "      while success:\n",
        "          success, image = vidObj.read()\n",
        "          if success:\n",
        "              yield image\n",
        "def im_plot(tensor):\n",
        "    image = tensor.cpu().numpy().transpose(1,2,0)\n",
        "    b,g,r = cv2.split(image)\n",
        "    image = cv2.merge((r,g,b))\n",
        "    image = image*[0.22803, 0.22145, 0.216989] +  [0.43216, 0.394666, 0.37645]\n",
        "    image = image*255.0\n",
        "    plt.imshow(image.astype(int))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K1gTbMuNu5i"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define image size and normalization parameters\n",
        "im_size = 112\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Define transformations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((im_size, im_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Define the paths to the videos\n",
        "path_to_videos = [\n",
        "  '/content/drive/MyDrive/testing/id1_id4_0006.mp4',\n",
        "  '/content/drive/MyDrive/testing/id1_id4_0007.mp4',\n",
        "  '/content/drive/MyDrive/testing/00000.mp4',\n",
        "  '/content/drive/MyDrive/testing/id0_id3_0002.mp4'\n",
        "]\n",
        "\n",
        "# Create the dataset for validation\n",
        "video_dataset = validation_dataset(path_to_videos, sequence_length=20, transform=train_transforms)\n",
        "\n",
        "# Load the model\n",
        "model = Model(num_classes=2).cuda()\n",
        "path_to_model = '/content/checkpoint.pt'  # Updated to your model path\n",
        "if os.path.exists(path_to_model):\n",
        "    model.load_state_dict(torch.load(path_to_model))\n",
        "    model.eval()\n",
        "else:\n",
        "    print(f\"Model file not found at {path_to_model}. Please check the path and try again.\")\n",
        "\n",
        "# Making predictions\n",
        "for video_path in path_to_videos:\n",
        "    print(f\"Processing video: {video_path}\")\n",
        "\n",
        "    # Find the index of the current video to get frames\n",
        "    video_index = path_to_videos.index(video_path)\n",
        "    frames_tensor = video_dataset[video_index]  # Accessing the frames for the specific video\n",
        "\n",
        "    prediction = predict(model, frames_tensor)  # Pass the correct frames_tensor\n",
        "    if prediction[0] == 1:\n",
        "        print(\"Prediction: REAL\")\n",
        "    else:\n",
        "        print(\"Prediction: FAKE\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}